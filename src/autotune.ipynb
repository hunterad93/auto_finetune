{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook can be used to create a fine-tuned model, using the OpenAI API. With gpt-4o as the teacher and gpt-4o-mini as the student.\n",
    "\n",
    "### Steps to run:\n",
    "1. In `utils.py`, set a path to an .env file containing an export OPENAI_API_KEY=\"\".\n",
    "2. Define a response model in the second code cell, see instructions above that cell.\n",
    "3. Set constants in the third code cell to control prompts, models, and other parameters.\n",
    "4. Change the pydantic name in the fourth code cell to the response model you defined.\n",
    "5. Run the cells in sequence down to the 11th code cell, the 11th code cell will set up a finetuning job, which may take a few hours to complete. OpenAI will email you when it's done if you have the setting enabled.\n",
    "6. The 12th code cell will compare the base mini model, the finetuned mini model, and the teacher model on the test set, using vector similarity to compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Pipeline Notebook\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Type, Any, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.batch_preparation import prepare_batch_file\n",
    "from src.batch_processing import upload_batch_file, create_batch_job, wait_for_batch_completion, process_batch_results\n",
    "from src.data_processing import prepare_finetuning_data, validate_finetuning_data\n",
    "from src.finetuning import prepare_and_start_finetuning\n",
    "from src.evaluation import evaluate_models, monitor_finetuning_job\n",
    "\n",
    "\n",
    "# Function to load prompts from a text file\n",
    "def load_prompts_from_file(file_path: str) -> List[str]:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return [line.strip() for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your response model below using a pydantic BaseModel. This is an easy way to use pythonic concepts when describing to the LLM how to structure its response. Copy the class to use when deploying the finetuned model, passing it into the response_model parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your response model (example)\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field(description=\"Overall sentiment of the text\")\n",
    "    intensity: float = Field(description=\"Strength of the sentiment, from 0.0 to 1.0\")\n",
    "    label: str = Field(description=\"Single word category describing the main focus of the sentiment (e.g., 'service', 'food', 'price')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running this cell takes the model you defined above and uses it to set the RESPONSE_MODEL variable, which is used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the most recently defined Pydantic model\n",
    "# the purpose of this is just to pass the above object to other functions\n",
    "# in this notebook, to save on needing to pass the object as a parameter manually.\n",
    "\n",
    "def get_latest_pydantic_model() -> Type[BaseModel]:\n",
    "    models = [cls for name, cls in globals().items() if isinstance(cls, type) and issubclass(cls, BaseModel) and cls != BaseModel]\n",
    "    if not models:\n",
    "        raise ValueError(\"No Pydantic model defined. Please define a model before running the pipeline.\")\n",
    "    return models[-1]\n",
    "\n",
    "RESPONSE_MODEL = get_latest_pydantic_model()\n",
    "MODEL_NAME: str = RESPONSE_MODEL.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is where you set the model parameters. Most importantly the system message for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths and model parameters\n",
    "SYSTEM_MESSAGE: str = \"You are a sentiment analysis model. You will be given a text and asked to analyze the sentiment of the text. You will return a sentiment, intensity, and label.\"\n",
    "LARGE_MODEL: str = \"gpt-4o-2024-08-06\"\n",
    "MINI_MODEL: str = \"gpt-4o-mini-2024-07-18\"\n",
    "MAX_TOKENS: int = 2000\n",
    "SUFFIX: str = f\"{MODEL_NAME}_v1\"  # Change this for different versions of your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the prompts file path to the path of your prompts file. Leave the other paths as they are, unless you need the intermediate outputs to go somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS_FILE_PATH: str = \"../data/prompts/sentiment_analysis_prompts.txt\"\n",
    "BATCH_INPUT_DIR: Path = Path(\"../data/batch_files/batch_inputs\")\n",
    "BATCH_OUTPUT_DIR: Path = Path(\"../data/batch_files/batch_outputs\")\n",
    "FINETUNE_DIR: Path = Path(\"../data/finetune_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 prompts from ../data/prompts/sentiment_analysis_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# Load prompts\n",
    "prompts: List[str] = load_prompts_from_file(PROMPTS_FILE_PATH)\n",
    "print(f\"Loaded {len(prompts)} prompts from {PROMPTS_FILE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input file created at: ../data/batch_files/batch_inputs/SentimentAnalysis_batch_input.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Prepare batch file\n",
    "batch_input_path = prepare_batch_file(\n",
    "    prompts=prompts,\n",
    "    response_model=RESPONSE_MODEL,\n",
    "    system_message=SYSTEM_MESSAGE,\n",
    "    model=LARGE_MODEL,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    save_dir=BATCH_INPUT_DIR,\n",
    "    filename_prefix=MODEL_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In theory this cell can take up to 24h to run, depending on the business of the batch API. In practice, it's usually done in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job created with ID: batch_SBzD9kac7okiv3iP8QRPfbAz\n",
      "Batch status: validating. Waiting 60 seconds...\n",
      "Batch status: in_progress. Waiting 60 seconds...\n",
      "Batch job batch_SBzD9kac7okiv3iP8QRPfbAz completed\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process batch\n",
    "batch_file_id = upload_batch_file(batch_input_path)\n",
    "batch_job_id = create_batch_job(batch_file_id)\n",
    "completed_batch = wait_for_batch_completion(batch_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch results saved to: ../data/batch_files/batch_outputs/SentimentAnalysis_batch_output_batch_SBzD9kac7okiv3iP8QRPfbAz.jsonl\n",
      "Batch output saved to: ../data/batch_files/batch_outputs/SentimentAnalysis_batch_output_batch_SBzD9kac7okiv3iP8QRPfbAz.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Process batch results\n",
    "batch_output_path = process_batch_results(\n",
    "    batch_job_id,\n",
    "    BATCH_OUTPUT_DIR,\n",
    "    f\"{MODEL_NAME}_batch_output\"\n",
    ")\n",
    "print(f\"Batch output saved to: {batch_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to: ../data/finetune_files/train/SentimentAnalysis_train.jsonl\n",
      "Testing data saved to: ../data/finetune_files/test/SentimentAnalysis_test.jsonl\n",
      "Training file created: ../data/finetune_files/train/SentimentAnalysis_train.jsonl\n",
      "Testing file created: ../data/finetune_files/test/SentimentAnalysis_test.jsonl\n",
      "Fine-tuning data is valid.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prepare fine-tuning data\n",
    "train_file, test_file = prepare_finetuning_data(\n",
    "    batch_input_path=batch_input_path,\n",
    "    batch_output_path=batch_output_path,\n",
    "    output_dir=FINETUNE_DIR,\n",
    "    output_filename_prefix=MODEL_NAME\n",
    ")\n",
    "print(f\"Training file created: {train_file}\")\n",
    "print(f\"Testing file created: {test_file}\")\n",
    "\n",
    "if validate_finetuning_data(train_file) and validate_finetuning_data(test_file):\n",
    "    print(\"Fine-tuning data is valid.\")\n",
    "else:\n",
    "    raise ValueError(\"Fine-tuning data is invalid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning file uploaded with ID: file-5OK1M9wo7nM9mnxdWX4AACaV\n",
      "Fine-tuning file uploaded with ID: file-qxwO65nGQKV0DK9AYymPbkJf\n",
      "Fine-tuning job created with ID: ftjob-5955i0ExbrHMNvh8ovrokdDL\n",
      "\n",
      "Fine-tuning job started with ID: ftjob-5955i0ExbrHMNvh8ovrokdDL\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Start fine-tuning\n",
    "job_id = prepare_and_start_finetuning(\n",
    "    training_file_path=train_file,\n",
    "    validation_file_path=test_file,\n",
    "    model=MINI_MODEL,\n",
    "    suffix=SUFFIX\n",
    ")\n",
    "\n",
    "print(f\"\\nFine-tuning job started with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job status: validating_files. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job status: running. Waiting 60 seconds...\n",
      "Fine-tuning job ftjob-5955i0ExbrHMNvh8ovrokdDL completed successfully.\n",
      "Fine-tuned model name: ft:gpt-4o-mini-2024-07-18:pathlabs:sentimentanalysis-v1:9wdfmppr\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_name = monitor_finetuning_job(job_id)\n",
    "print(f\"Fine-tuned model name: {finetuned_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for the finetune job to complete before running the next cell. The next cell will compare the vector similarity of the responses of three models: the base mini model, the finetuned mini model, and the teacher model.\n",
    "\n",
    "If the environment variable for finetuned_model_name is lost through the course of the notebook, you can manually set it in the next cell. If this happens rerun the first 6 code cells before running the next 2 cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch job created with ID: batch_dDMpUs1XoHB8J5GbzdWQBH6V\n",
      "Batch status: validating. Waiting 60 seconds...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Batch job batch_dDMpUs1XoHB8J5GbzdWQBH6V failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinetuned_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinetuned_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_mini_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMINI_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlarge_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLARGE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/evaluation_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESPONSE_MODEL\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, result_path \u001b[38;5;129;01min\u001b[39;00m evaluation_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/auto_finetune/src/evaluation.py:190\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m(validation_file, finetuned_model, base_mini_model, large_model, max_tokens, save_dir, response_model)\u001b[0m\n\u001b[1;32m    182\u001b[0m evaluation_data \u001b[38;5;241m=\u001b[39m prepare_evaluation_data(validation_file)\n\u001b[1;32m    184\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m: finetuned_model,\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_mini\u001b[39m\u001b[38;5;124m\"\u001b[39m: base_mini_model,\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m: large_model\n\u001b[1;32m    188\u001b[0m }\n\u001b[0;32m--> 190\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_models_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Compare model outputs\u001b[39;00m\n\u001b[1;32m    199\u001b[0m similarities \u001b[38;5;241m=\u001b[39m compare_model_outputs(results)\n",
      "File \u001b[0;32m~/Documents/auto_finetune/src/evaluation.py:86\u001b[0m, in \u001b[0;36mrun_models_evaluation\u001b[0;34m(evaluation_data, models, max_tokens, save_dir, response_model)\u001b[0m\n\u001b[1;32m     83\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m create_batch_job(file_id)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Wait for job completion and process results\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mwait_for_batch_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m results_path \u001b[38;5;241m=\u001b[39m process_batch_results(batch_id, save_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_output_all_models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Split the results into separate files for each model\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/auto_finetune/src/batch_processing.py:35\u001b[0m, in \u001b[0;36mwait_for_batch_completion\u001b[0;34m(batch_id, polling_interval)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpired\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch job \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolling_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(polling_interval)\n",
      "\u001b[0;31mException\u001b[0m: Batch job batch_dDMpUs1XoHB8J5GbzdWQBH6V failed"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluation_results = evaluate_models(\n",
    "    validation_file=test_file,\n",
    "    finetuned_model=finetuned_model_name,\n",
    "    base_mini_model=MINI_MODEL,\n",
    "    large_model=LARGE_MODEL,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    save_dir=Path(\"../data/evaluation_results\"),\n",
    "    response_model=RESPONSE_MODEL\n",
    ")\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for model, result_path in evaluation_results[\"results\"].items():\n",
    "    print(f\"{model}: {result_path}\")\n",
    "\n",
    "print(\"\\nModel similarities:\")\n",
    "for comparison, similarity in evaluation_results[\"similarities\"].items():\n",
    "    print(f\"{comparison}: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
